{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f2a8270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 0: IMPORTS (Like ML libraries: pandas, sklearn)\n",
    "from langchain_community.document_loaders import TextLoader  # Loads text docs (like pd.read_csv)\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Splits docs into chunks (like train_test_split)\n",
    "from langchain_community.vectorstores import FAISS  # Vector \"model\" store (like sklearn's fit/transform)\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # Vectorizer (like StandardScaler)\n",
    "from langchain_core.prompts import ChatPromptTemplate  # Prompt \"template\" (like model formula)\n",
    "from langchain_core.output_parsers import StrOutputParser  # \"Predict\" cleaner (like predict_proba → label)\n",
    "from langchain_core.runnables import RunnablePassthrough  # Passes data through (like X = X, y = y)\n",
    "from langchain_ollama import ChatOllama  # LLM \"model\" (like LinearRegression)\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory  # Memory \"store\" (like session df)\n",
    "from langchain_core.runnables import RunnableWithMessageHistory  # Memory wrapper (like fit with history)\n",
    "from langchain_core.tools import tool  # Tool def (like custom feature)\n",
    "from langchain_core.messages import AIMessage, HumanMessage  # Message types (like data rows)\n",
    "import redis  # Caching \"store\" (like joblib dump)\n",
    "import json  # Serialize for Redis (like pickle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb4c092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM \"model\" (like model = LinearRegression())\n",
    "llm = ChatOllama(model=\"mistral\", temperature=0.5)\n",
    "\n",
    "# Redis for caching (like cache = joblib.Memory(location='cache'))\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# STEP 1: DATA LOADING & EDA (Like pd.read_csv + df.describe())\n",
    "print(\"=== STEP 1: DATA LOADING & EDA ===\")\n",
    "loader = TextLoader(\"E:\\LTI\\ollama-env\\lc-proj\\sample_docs\\gdpr_guideline.txt\")  # What: Loads full doc as Document (text + metadata). Why: Raw input for RAG (like df = pd.read_csv). How: Like reading file.\n",
    "docs = loader.load()  # What: List of Document objects. Why: Standard LangChain format. How: Connects to splitter.\n",
    "\n",
    "print(f\"Loaded {len(docs)} docs.\")\n",
    "print(\"Sample content:\", docs[0].page_content[:200])  # EDA: Preview (like df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d0aba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: DATA PREP - CHUNKING (Like train_test_split + scaling)\n",
    "print(\"\\n=== STEP 2: DATA PREP - CHUNKING ===\")\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)  # What: Splits doc into overlapping chunks. Why: Docs too long for LLM/embed (token limit); overlap preserves context (like CV split). How: Recursive (sentences > words > chars).\n",
    "splits = splitter.split_documents(docs)  # What: List of smaller Documents. Why: Embeddable units. How: Input to embeddings (like X_train = scaler.fit_transform(X)).\n",
    "\n",
    "print(f\"Created {len(splits)} chunks.\")  # EDA: Check split (like len(X_train))\n",
    "for i, chunk in enumerate(splits[:2]):\n",
    "    print(f\"Chunk {i+1} (len {len(chunk.page_content)} chars): {chunk.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae631361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STEP 3: MODEL BUILD - EMBEDDING & INDEXING ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_11768\\590570736.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # What: Text → vectors (384-dim). Why: Semantic search (similar texts close in vector space). How: Pre-trained model (like fit on chunks).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index built with {len(splits)} vectors.\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: \"MODEL BUILD\" - EMBEDDING & INDEXING (Like fit(transform))\n",
    "print(\"\\n=== STEP 3: MODEL BUILD - EMBEDDING & INDEXING ===\")\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")  # What: Text → vectors (384-dim). Why: Semantic search (similar texts close in vector space). How: Pre-trained model (like fit on chunks).\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)  # What: Builds FAISS index (vector store). Why: Fast similarity search (cosine). How: Embeds chunks, stores for retrieval (like model.fit(X)).\n",
    "\n",
    "retriever = vectorstore.as_retriever(k=3)  # What: Wrapper for top-k search. Why: Easy LCEL plug-in. How: query → embed → top matches (like predict(X_test)).\n",
    "\n",
    "print(\"Index built with {len(splits)} vectors.\")  # EDA: Verify (like model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e7f3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ChatPromptTemplate.from_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dca561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: \"PREDICT\" - RETRIEVE + AUGMENT + GENERATE (Like model.predict)\n",
    "print(\"\\n=== STEP 4: PREDICT - RETRIEVE + AUGMENT + GENERATE ===\")\n",
    "prompt = ChatPromptTemplate.from_template(\"FAQ Bot: Context: {context}\\nQuestion: {question}\\nAnswer:\")  # What: Prompt template. Why: Augments with retrieved context (RAG core). How: {context} = chunks, {question} = input.\n",
    "parser = StrOutputParser()  # What: Strips AIMessage to string. Why: Clean output. How: Post-LLM (like to_label).\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # What: Dict runnable. Why: Retrieve chunks as 'context', pass question. How: Parallel (retrieve + query).\n",
    "    | prompt  # Augment: Stuff context into template.\n",
    "    | llm  # Generate: LLM reasons over augmented prompt.\n",
    "    | parser  # Clean: String output.\n",
    ")\n",
    "\n",
    "query = \"gdpr rules in eu?\"  # Test input (like X_test)\n",
    "response = chain.invoke(query)  # What: Runs pipeline. Why: End-to-end predict. How: LCEL pipe executes steps.\n",
    "print(\"RAG Response:\", response)  # Output: Grounded answer (e.g., \"Up to $50k for breaches...\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c48b08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: INTEGRATE MEMORY (Retain user history for recurring issues)\n",
    "print(\"\\n=== STEP 5: INTEGRATE MEMORY ===\")\n",
    "store = {}  # What: Session dict. Why: Per-user history. How: Key = session_id.\n",
    "def get_session_history(session_id: str):  # What: Loader. Why: Scopes memory. How: InMemory for messages.\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "memory_chain = RunnableWithMessageHistory(  # What: Wrapper. Why: Auto-loads/appends history. How: Wraps chain.\n",
    "    chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"question\",  # Maps query to HumanMessage.\n",
    "    history_messages_key=\"context\"  # Injects history as context (augment with past).\n",
    ")\n",
    "\n",
    "config = {\"configurable\": {\"session_id\": \"user1\"}}  # What: Session. Why: Scopes. How: Passed to invoke.\n",
    "response1 = memory_chain.invoke({\"question\": \"Outlook basics?\"}, config)  # Turn 1\n",
    "print(\"Turn 1:\", response1)\n",
    "\n",
    "response2 = memory_chain.invoke({\"question\": \"More on crashes from basics?\"}, config)  # Turn 2\n",
    "print(\"Turn 2 (memory):\", response2)  # Output: References \"basics\" from turn 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28516598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: INTEGRATE TOOLS (Enrich with external action)\n",
    "print(\"\\n=== STEP 6: INTEGRATE TOOLS ===\")\n",
    "@tool  # What: Decorator. Why: LLM-callable. How: Schema from docstring.\n",
    "def kb_lookup(query: str) -> str:  # What: Tool. Why: Mock KB enrich. How: LLM binds.\n",
    "    \"\"\"Mock IT KB lookup for fixes.\"\"\"\n",
    "    return f\"KB for '{query}': Restart service or update patch.\"\n",
    "\n",
    "llm_with_tools = llm.bind_tools([kb_lookup])  # What: Bind. Why: LLM decides call. How: Outputs tool_calls.\n",
    "\n",
    "tool_chain = (\n",
    "    {\"context\": hybrid_retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm_with_tools  # Tool-enabled LLM.\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response_tool = tool_chain.invoke(\"Outlook crash? Fix?\")  # What: Invoke. Why: Triggers if relevant. How: LLM reasons.\n",
    "print(\"With Tool:\", response_tool)  # Output: RAG + \"KB: Restart...\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65064df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual resolve (like post-process)\n",
    "if hasattr(response_tool, 'tool_calls') and response_tool.tool_calls:\n",
    "    tool_res = kb_lookup.invoke(response_tool.tool_calls[0]['args'])\n",
    "    response_tool += f\"\\nTool KB: {tool_res}\"  # Augment.\n",
    "print(\"Resolved:\", response_tool)\n",
    "\n",
    "# STEP 7: PERSISTENCE - SAVE INDEX (Like joblib.dump)\n",
    "print(\"\\n=== STEP 7: PERSISTENCE - SAVE INDEX ===\")\n",
    "vectorstore.save_local(\"ticket_db\")  # What: Disk save. Why: Survives restarts. How: FAISS folder (index.faiss + index.pkl).\n",
    "\n",
    "# STEP 8: CACHING WITH REDIS (Optional - Like joblib.Memory)\n",
    "print(\"\\n=== STEP 8: CACHING WITH REDIS (Optional) ===\")\n",
    "cache_key = f\"rag:{query}\"  # What: Key. Why: Cache hits fast. How: Query as key.\n",
    "cached = r.get(cache_key)  # Get.\n",
    "if cached:\n",
    "    print(\"Cache Hit:\", cached.decode())\n",
    "else:\n",
    "    response = chain.invoke(query)  # Miss: Run chain.\n",
    "    r.setex(cache_key, 3600, response)  # Set with 1h TTL.\n",
    "    print(\"Cache Miss - Stored:\", response)\n",
    "\n",
    "# STEP 9: EVALUATE (Like model.score)\n",
    "print(\"\\n=== STEP 9: EVALUATE ===\")\n",
    "retrieved = hybrid_retriever.invoke(query)  # Contexts.\n",
    "mock_ground = \"Outlook crash: Restart service.\"  # Mock truth.\n",
    "score = 1 if \"restart\" in response.lower() else 0  # Mock relevance.\n",
    "print(f\"Eval Score: {score}/1 - {'Pass' if score == 1 else 'Fail'}\")\n",
    "\n",
    "# Full test with memory/tool/cache\n",
    "print(\"\\n=== FULL TEST ===\")\n",
    "config = {\"configurable\": {\"session_id\": \"user2\"}}\n",
    "response_full = memory_chain.invoke({\"question\": \"Outlook basics? Crashes?\"}, config)\n",
    "print(\"Full Pipeline:\", response_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bd45fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dcc086",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ollama-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
