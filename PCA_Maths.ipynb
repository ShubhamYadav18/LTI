{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035616,
     "end_time": "2022-04-19T11:38:02.075585",
     "exception": false,
     "start_time": "2022-04-19T11:38:02.039969",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Dimensionality Reduction\n",
    "\n",
    "In various practical applications, even though the data resides in a high-dimensional space, the intrinsic dimensionality—referred to as the true dimensionality—can often be significantly lower.\n",
    "\n",
    "For instance, within a three-dimensional space, data points might cluster around a straight line, the circumference of a circle, or the graph of a parabola, positioned arbitrarily in R^3.\n",
    "\n",
    "The image below demonstrates these three scenarios. The primary purpose of understanding the lower-dimensional structure associated with a given dataset has become increasingly significant in big data processing and analysis, especially in fields like computer vision, robotics, medical imaging, and computational neuroscience.\n",
    "\n",
    "![img](https://i.imgur.com/IkFGd65.png)\n",
    "\n",
    "The data align closely to (A) a straight line, (B) the circumference of a circle, and (C) the graph of a parabola within the three-dimensional space. In all these cases, the intrinsic dimensionality of the data equates to one. In scenario (A), the data clusters around a (translated/affine) linear subspace, while in scenarios (B) and (C), they cluster around one-dimensional manifolds.\n",
    "\n",
    "#### Various Techniques for Dimensionality Reduction:\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Linear Discriminant Analysis (LDA)\n",
    "- Generalized Discriminant Analysis (GDA)\n",
    "\n",
    "Dimensionality reduction can be linear or non-linear, based on the chosen method. The primary linear technique, known as Principal Component Analysis (PCA), is discussed further.\n",
    "\n",
    "# When to Utilize Dimensionality Reduction\n",
    "\n",
    "Consider PCA as an exploratory technique to investigate and comprehend your system before proceeding with other actions. Dimensionality reduction inherently leads to some information loss, similar to image compression. As a result, it might decrease the prediction quality of your model. However, depending on the data, the impact could be neutral or even positive in some cases (e.g., noisy data). Typically, the main advantage lies in accelerating training times.\n",
    "\n",
    "Dimensionality reduction can also assist in mitigating multicollinearity in certain instances. This issue arises primarily when parameter estimation, such as causal analysis, is involved. If, for example, you use a multiple linear regression model to estimate the effect of several regressors on a dependent variable, multicollinearity hampers accurate parameter estimation, making it impossible to identify the effect of each regressor.\n",
    "\n",
    "Nevertheless, in numerous machine learning applications, the focus is on prediction rather than parameter identification. If some variables are highly correlated, redundant information might exist in your data, which doesn't necessarily pose a problem in terms of prediction quality.\n",
    "\n",
    "---\n",
    "\n",
    "# Data Preparation for Dimensionality Reduction\n",
    "\n",
    "There's no one-size-fits-all technique for dimensionality reduction, and no strict mapping of techniques to specific problems.\n",
    "\n",
    "Instead, the most effective approach involves systematic controlled experiments to determine which dimensionality reduction techniques, when combined with your chosen model, yield optimal performance on your dataset.\n",
    "\n",
    "Methods rooted in linear algebra and manifold learning often assume that all input features share the same scale or distribution. Hence, it's recommended to normalize or standardize data before applying these methods if input variables exhibit varying scales or units.\n",
    "\n",
    "# Comparing Feature Selection, PCA, and Clustering for Dimension Reduction\n",
    "\n",
    "![img](https://i.imgur.com/1KUD3gN.png)\n",
    "\n",
    "The first two methods, Feature Selection and PCA, reduce the dimensionality of the feature space, which translates to reducing the number of rows in a data matrix. However, these methods function differently. While feature selection directly retains rows from the original matrix, PCA employs the geometry of the feature space to generate a new data matrix based on a lower-dimensional version of the data. In contrast, K-means reduces the dimensionality of the data, decreasing the number of data points or columns in the input data matrix. This is accomplished by finding a small number of new average representatives or \"centroids\" of the input data, constructing a new data matrix where these centroids form the fewer columns (absent in the original data matrix).\n",
    "\n",
    "# Mean Vector\n",
    "\n",
    "The initial step in multivariate data analysis involves computing the mean vector and the variance-covariance matrix.\n",
    "\n",
    "The sample mean comprises elements that are the sample means of individual random variables. Essentially, each element represents the arithmetic average of observed values for a specific variable.\n",
    "\n",
    "![img](https://i.imgur.com/XG2Yag7.png)\n",
    "\n",
    "For instance, consider the following two vectors:\n",
    "\n",
    "x1 = [2.2, 4.2, …]\n",
    "\n",
    "x2 = [1.2, 3.2, …]\n",
    "\n",
    "The formula for calculating `x_mean = 1/2(x1+x2)` results in:\n",
    "\n",
    "= 0.5* [3.4, 7.4, ..]\n",
    "\n",
    "= [1.7,3.7, .. ]\n",
    "\n",
    "This calculation involves summing elements at the ith index of the first array and the corresponding index of the second array. This further signifies that each array can be treated as a vector, with its indices representing dimensions.\n",
    "\n",
    "# Covariance Matrix\n",
    "\n",
    "The Covariance Matrix consists of covariances between pairs of variables positioned in other matrix positions.\n",
    "\n",
    "To clarify the distinction between covariance and variance: Variance quantifies the variation of a single random variable (like height in a population), whereas covariance gauges how much two random variables co-vary (like height and weight in a population). Variance is computed as follows:\n",
    "\n",
    "![img](https://i.imgur.com/CnnSCxv.png)\n",
    "\n",
    "### Definition of Co-Variance for 2 Scalar Values from [Wikipedia](https://en.wikipedia.org/wiki/Covariance)\n",
    "\n",
    "For two jointly distributed real-valued random variables X and Y with finite second moments, covariance is the expected value (or mean) of the product of their deviations from their individual expected values:\n",
    "\n",
    "![img](https://i.imgur.com/jE6qkN7.png)\n",
    "\n",
    "where E[X] is the expected value of X, also known as the mean of X.\n",
    "\n",
    "### Definition of Co-Variance for Matrix Form of Data - i.e., When X (as defined in the above formula) is a Vector\n",
    "\n",
    "![img](https://i.imgur.com/eS35C7X.png)\n",
    "\n",
    "Notably, the transpose is significant in this matrix form because we're multiplying two vectors.\n",
    "\n",
    "Alternatively, the formula for the Covariance Matrix is:\n",
    "\n",
    "![img](https://i.imgur.com/AZ10i59.png)\n",
    "\n",
    "Emphasizing the last point from above, if the columns of the original Matrix are standardized—meaning column vectors have zero mean and standard deviation of 1—the formula for the Covariance of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:02.153489Z",
     "iopub.status.busy": "2022-04-19T11:38:02.152929Z",
     "iopub.status.idle": "2022-04-19T11:38:02.180994Z",
     "shell.execute_reply": "2022-04-19T11:38:02.180355Z",
     "shell.execute_reply.started": "2022-04-19T11:28:48.084211Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.071658,
     "end_time": "2022-04-19T11:38:02.181123",
     "exception": false,
     "start_time": "2022-04-19T11:38:02.109465",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "A = np.array([[1, 3, 5], [5, 4, 1], [3, 8, 6]])\n",
    "# A = np.array([[ 3, 5], [4, 1], [8, 6]])\n",
    "print(A)\n",
    "\n",
    "# array([[1, 3, 5],\n",
    "#        [5, 4, 1],\n",
    "#        [3, 8, 6]])\n",
    "\n",
    "cov_matrix = np.cov(A, rowvar=False, bias=True )\n",
    "cov_matrix\n",
    "\n",
    "# array([[ 2.66666667, 0.66666667, -2.66666667],\n",
    "#        [ 0.66666667, 4.66666667, 2.33333333],\n",
    "#        [-2.66666667, 2.33333333, 4.66666667]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034312,
     "end_time": "2022-04-19T11:38:02.283297",
     "exception": false,
     "start_time": "2022-04-19T11:38:02.248985",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### The diagonal elements of this matrix are the variances of the variables, and the off-diagonal elements are the covariances between the variables.\n",
    "\n",
    "In below image all green highlighted are the variances and the rest (red highlighted) are the co-variances\n",
    "\n",
    "![img](https://i.imgur.com/8PJ5lMV.png)\n",
    "\n",
    "## Note, the Dimension of the Co-Variance Matrix will always be a Square Matrix -\n",
    "\n",
    "A Square Matrix is one with same number of rows and columns). Because they capture the covariance between any Variable and any other Variable. So we need all the Variables (Features in a dataset) on the columns and also all the variables on the rows in the resultant Co-Variance Matrix. Thats why in the above image I have put all of x1, x2, x3 in both the columns and rows.\n",
    "\n",
    "#### So, m-dimensional data will result in mxm covariance matrix\n",
    "\n",
    "Basically for three variables (or 3 dimensions of a dataset or 3 features of a dataset) X, Y and Z we will get the Covariance Matrix as following\n",
    "\n",
    "![img](https://i.imgur.com/AAq7ZC1.png)\n",
    "\n",
    "![img](https://i.imgur.com/j0vnWlR.png)\n",
    "\n",
    "Thus,\n",
    "\n",
    "- 2.66666667 is the variance of the x1 variable,\n",
    "\n",
    "- 0.66666667 is the covariance between x1 and x2 variables,\n",
    "\n",
    "- -2.66666667 is the covariance between x1 and x3 variables,\n",
    "\n",
    "- 4.66666667 is the variance of x2,\n",
    "\n",
    "- 2.33333333 is the covariance between x2 and x3 and\n",
    "\n",
    "- 4.66666667 is the variance of x3.\n",
    "\n",
    "#### The mean vector is often referred to as the centroid and the variance-covariance matrix as the dispersion or dispersion matrix. Also, the terms variance-covariance matrix and covariance matrix are used interchangeably.\n",
    "\n",
    "## Properties of Co-Variance Matrix\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Principal Component Analysis\n",
    "\n",
    "PCA stands for Principal Component Analysis. Here is what Wikipedia says about PCA.\n",
    "\n",
    "\"Given a collection of points in two, three, or higher dimensional space, a “best fitting” line can be defined as one that minimizes the average squared distance from a point to the line. The next best-fitting line can be similarly chosen from directions perpendicular to the first. Repeating this process yields an orthogonal basis in which different individual dimensions of the data are uncorrelated. These basis vectors are called principal components, and several related procedures principal component analysis (PCA).\"\n",
    "\n",
    "So, PCA means extracting the components (features) which are most effective in forecasting the target variable in a dataset, and discarding the redundant features. Thus, we calculate the Principal Components to achieve Dimensionality reduction.\n",
    "\n",
    "PCA ( aka Karhunen–Loève transform in Mathematics ) is among the oldest and most widely used methods for dimensionality reduction [105]. The assumption underlying PCA, as well as any dimensionality reduction technique, is that the observed data are generated by a system or process that is driven by a (relatively) small number of latent (not directly observed) variables. The goal is to learn this latent structure.\n",
    "\n",
    "Here's a visual representation of PCA being applied to a simple 2-D data set (left, in blue)\n",
    "\n",
    "![img](https://i.imgur.com/lKK6Gts.png)\n",
    "\n",
    "As we can see above dimension reduction via PCA retains much of the structure of the original data. The ideal subspace is shown in black in the left panel, and the data projected onto this subspace is shown in blue on the right. (bottom panels)\n",
    "\n",
    "Before using PCA, we must standardize our dataset. The method won’t work if we have entirely different scales for our data. Standardization helps to avoid biased outcomes.\n",
    "\n",
    "For example, suppose there are three variables – v1 in a 1-10 range, v2 in a 10-100 range, and v3 in a 100,000 to 1,000,000 range. If we just go ahead and compute an output using these predictors, we’ll get a heavily biased result because the third variable will have a disproportionately large impact on the output value.\n",
    "\n",
    "#### Step-1 So, Before applying PCA, we must ensure that all our attributes (dimensions) are centered around zero and have a standard deviation of 1. So, the first thing we do is calculate the means for all of them. Then, we center the values in each column (to do so we subtract the mean column value) and calculate the covariance matrix for the resulting centered matrix.\n",
    "\n",
    "Now, we can calculate our first principal component.\n",
    "\n",
    "#### Step-2 - Next, we utilize one of the methods for breaking up matrices (eigendecomposition or singular value decomposition) to turn our matrix into a list of eigenvectors that will define directions of axes in the new subspace and eigenvalues that will show the magnitude of those directions.\n",
    "\n",
    "We sort the vectors by their eigenvalues in decreasing order and thus get a ranking of the principal components.\n",
    "\n",
    "#### Step-3 - Now finally, we can ditch some of the dimensions and project our dataset into a reduced subspace without losing important information about the original dataset. And since new axes capture the maximum variance in the data, any clustering algorithm we’re going to apply next will have an easier time grouping the data instances.\n",
    "\n",
    "Lets look at it visually,\n",
    "\n",
    "Imagine this is our dataset that we’re trying to cluster; we only have two dimensions.\n",
    "\n",
    "![img](https://i.imgur.com/CUWkBzi.png)\n",
    "\n",
    "If we project the data onto the horizontal axis (our attribute 1) we won’t see much spread; it will show a nearly equal distribution of the observations.\n",
    "\n",
    "![img](https://i.imgur.com/s5O8hYn.png)\n",
    "\n",
    "Attribute 2, obviously, isn’t hugely helpful either.\n",
    "\n",
    "![img](https://i.imgur.com/6VnyFk7.png)\n",
    "\n",
    "As the data points in this case are spreading diagonally so we need a new line that would better capture this.\n",
    "\n",
    "![img](https://i.imgur.com/B9MtmIg.png)\n",
    "\n",
    "#### This axis is our first Principle Component (PC) – a new direction (we can imagine it to be an attribute) that maximizes the variance of the data (and thus the clusters become much more obvious.) Besides maximizing the spread, this first PC sits through the direction in the data that minimizes the distances between all the points and the new axis.\n",
    "\n",
    "#### The second PC must represent the second maximum amount of variance; it’s going to be a line that’s orthogonal to our first axis.\n",
    "\n",
    "![img](https://i.imgur.com/ocDf36C.png)\n",
    "\n",
    "Because of the underlying PCA-Math being based on eigenvectors and eigenvalues, new principal components will always come out orthogonal to the ones before them. **Eigenvectors**, simply, are vectors that aren’t knocked off of their span by a linear transformation; they can hold on to their original direction while being stretched, shrunk or reversed. Eigenvalues are factors by which these special vectors are scaled.\n",
    "\n",
    "\n",
    "\n",
    "# Key concept of Preserving the Variance by Projection of Data\n",
    "\n",
    "#### Before we can project the training set onto a lower-dimensional hyperplane, we first need to choose the right hyperplane. For example, in the below figure, a simple 2D dataset is represented on the left, along with three different axes (i.e., 1D hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As we can see, the projection onto the solid line preserves the maximum variance, while the projection onto the dotted line preserves very little variance and the projection onto the dashed line preserves an intermediate amount of variance.\n",
    "\n",
    "![img](https://i.imgur.com/Fp8kqgJ.png)\n",
    "\n",
    "#### It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the basic idea behind PCA.\n",
    "\n",
    "PCA identifies the axis that accounts for the largest amount of variance in the train-ing set. In above figure, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line (i.e. the orthogonal one). If it were a higher-dimensional data‐set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many axes as the number of dimensions in the dataset. The ith axis is called the ith principal component (PC) of the data. In above figure, the first PC is the axis on which vector c1 lies, and the second PC is the axis on which vector c2 lies. In case of 3-D dataset the first two PCs are the orthogonal axes on which the two arrows lie, on the plane, and the third PC is the axis orthogonal to that plane, like in the below figure.\n",
    "\n",
    "![img](https://i.imgur.com/wEHwUMl.png)\n",
    "\n",
    "\n",
    "For each principal component, PCA finds a zero-centered unit vector pointing in the direction of the PC. Since two opposing unit vectors lie on the same axis, the direction of the unit vectors returned by PCA is not stable: if you perturb the training set slightly and run PCA again, the unit vectors may point in the oppo‐\n",
    "site direction as the original vectors. However, they will generally still lie on the same axes. In some cases, a pair of unit vectors may even rotate or swap (if the variances along these two axes are close), but the plane they define will generally remain the same.\n",
    "\n",
    "\n",
    "# Eigenvalues and Eigenvectors\n",
    "Many problems present themselves in terms of an eigenvalue problem:\n",
    "\n",
    "###  A·v=λ·v\n",
    "\n",
    "In this equation A is an n-by-n matrix, v is a non-zero n-by-1 vector and λ is a scalar (which may be either real or complex).  Any value of λ for which this equation has a solution is known as an eigenvalue of the matrix A.\n",
    "\n",
    "To begin, let $v$ be a vector (shown as a point) and $A$ be a matrix with columns $a_1$ and $a_2$ (shown as arrows). If we multiply $v$ by $A$, then $A$ sends $v$ to a new vector $Av$.\n",
    "\n",
    "\n",
    "If you can draw a line through the three points $(0,0)$, $v$ and $Av$, then $Av$ is just $v$ multiplied by a number $\\lambda$; that is, $Av = \\lambda v$. In this case, we call $\\lambda$ an eigenvalue and $v$ an eigenvector.\n",
    "\n",
    "This means that the linear transformation A on vector **v** is completely defined by λ.\n",
    "\n",
    "In the linear algebra literature and software, it is often a convention that eigenvalues are sorted in descending order, so that the largest\n",
    "eigenvalue and associated eigenvector are called the first eigenvalue and its associated eigenvector, and the second largest called the second eigenvalue and its associated eigenvector, and so on.\n",
    "\n",
    "An eigenvector is a vector whose direction remains unchanged when a linear transformation is applied to it.\n",
    "\n",
    "\n",
    "### Usefulness of Eigenvalues and Eigenvectors\n",
    "\n",
    "If you keep multiplying $v$ by $A$, you get a sequence ${ v, Av, A^2v,}$ etc. Eigenspaces attract that sequence and eigenvalues tell you whether it ends up at $(0,0)$ or far away. Therefore, eigenvectors/values tell us about systems that evolve step-by-step.\n",
    "\n",
    "![img]https://i.imgur.com/1hXen7o.png[/img]\n",
    "\n",
    "### Properties of eigenvalues and eigenvectors\n",
    "\n",
    "- Let A be a $K\times K$ square matrix. A scalar $lambda $ is an eigenvalue of A if and only if it is an eigenvalue of $A^{intercal }$.\n",
    "\n",
    "-  For symmetric matrix, the eigenvectors are always orthogonal\n",
    "\n",
    "In general, for any matrix, the eigenvectors are NOT always orthogonal. But for a special type of matrix, symmetric matrix, the eigenvalues are always real and the corresponding eigenvectors are always orthogonal.\n",
    "\n",
    "For any matrix M with n rows and m columns, M multiplies with its transpose, either M*M' or M'M, results in a symmetric matrix, so for this symmetric matrix, the eigenvectors are always orthogonal.\n",
    "\n",
    "In the application of PCA, a dataset of n samples with m features is usually represented in a n* m matrix D. The variance and covariance among those m features can be represented by a m*m matrix D'*D, which is symmetric (numbers on the diagonal represent the variance of each single feature, and the number on row i column j represents the covariance between feature i and j). The PCA is applied on this symmetric matrix, so the eigenvectors are guaranteed to be orthogonal.\n",
    "\n",
    "\n",
    "#### During Steps PCA calculation steps we need the Eigenvectors as below\n",
    "\n",
    "- Step-A => Standardize the Columns\n",
    "\n",
    "- Step-B => Build the Covariance-Matrix S which is $(X^T * X)$\n",
    "\n",
    "- Step-C => Get the EigneVectors of this Covariance-Matrix S. These will be sorted from largest λ and downwards.\n",
    "\n",
    "- Step-D => And my **u1** is the Eigenvector **v1** which is the largest Eigenvector corresponding to the largest Eigenvalue\n",
    "\n",
    "## Important Note - PCA assumes that the dataset is centered around the origin.\n",
    "\n",
    "As we will see, Scikit-Learn’s PCA classes take care of centering the data for you. If you implement PCA yourself, or if you use other libraries, don’t forget to center the data first\n",
    "\n",
    "# Now I will Calculate the PCA for MNIST data quite manually using the steps described earlier, which are\n",
    "\n",
    "- Step-A => Standardize the Columns\n",
    "\n",
    "- Step-B => Build the Covariance-Matrix S which is $(X^T * X)$\n",
    "\n",
    "- Step-C => Get the EigneVectors of this Covariance-Matrix S. These will be sorted from largest λ and downwards. And my **u1** is the Eigenvector **v1** which is the largest Eigenvector corresponding to the largest Eigenvalue\n",
    "\n",
    "- Step-D => Now I need to project the original data sample on the plane formed by two principle eigenvectors by vector-vector multiplication.\n",
    "\n",
    "### The training dataset is 42,000 small square 28×28 pixel grayscale images of handwritten single digits between 0 and 9. The task is to classify a given image of a handwritten digit into one of 10 classes representing integer values from 0 to 9, inclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:02.357466Z",
     "iopub.status.busy": "2022-04-19T11:38:02.356819Z",
     "iopub.status.idle": "2022-04-19T11:38:03.262121Z",
     "shell.execute_reply": "2022-04-19T11:38:03.262592Z",
     "shell.execute_reply.started": "2022-04-19T11:28:48.132162Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.945066,
     "end_time": "2022-04-19T11:38:03.262800",
     "exception": false,
     "start_time": "2022-04-19T11:38:02.317734",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:03.338712Z",
     "iopub.status.busy": "2022-04-19T11:38:03.338122Z",
     "iopub.status.idle": "2022-04-19T11:38:08.553156Z",
     "shell.execute_reply": "2022-04-19T11:38:08.553694Z",
     "shell.execute_reply.started": "2022-04-19T11:28:49.127055Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 5.256063,
     "end_time": "2022-04-19T11:38:08.553841",
     "exception": false,
     "start_time": "2022-04-19T11:38:03.297778",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_org = pd.read_csv('train.csv')\n",
    "\n",
    "print(\"train_df_org shape \", train_df_org.shape) #  (1000, 785)\n",
    "\n",
    "train_df_org.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:08.790536Z",
     "iopub.status.busy": "2022-04-19T11:38:08.790016Z",
     "iopub.status.idle": "2022-04-19T11:38:08.795120Z",
     "shell.execute_reply": "2022-04-19T11:38:08.794690Z",
     "shell.execute_reply.started": "2022-04-19T11:28:55.050054Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.04471,
     "end_time": "2022-04-19T11:38:08.795217",
     "exception": false,
     "start_time": "2022-04-19T11:38:08.750507",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = train_df_org['label']\n",
    "labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:08.949190Z",
     "iopub.status.busy": "2022-04-19T11:38:08.948337Z",
     "iopub.status.idle": "2022-04-19T11:38:08.951760Z",
     "shell.execute_reply": "2022-04-19T11:38:08.952200Z",
     "shell.execute_reply.started": "2022-04-19T11:28:55.065975Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.121629,
     "end_time": "2022-04-19T11:38:08.952322",
     "exception": false,
     "start_time": "2022-04-19T11:38:08.830693",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_for_pca = train_df_org.drop(['label'], axis=1)\n",
    "train_df_for_pca.shape\n",
    "# In pandas, drop( ) function is used to remove column(s).axis=1 tells Python that you want to apply function on columns instead of rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.036296,
     "end_time": "2022-04-19T11:38:09.024767",
     "exception": false,
     "start_time": "2022-04-19T11:38:08.988471",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Step-A: Data-preprocessing: Standardizing the train dataframe\n",
    "#### `(x - mean) / s.d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:09.109649Z",
     "iopub.status.busy": "2022-04-19T11:38:09.108605Z",
     "iopub.status.idle": "2022-04-19T11:38:09.844457Z",
     "shell.execute_reply": "2022-04-19T11:38:09.843614Z",
     "shell.execute_reply.started": "2022-04-19T11:28:55.158255Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.783843,
     "end_time": "2022-04-19T11:38:09.844580",
     "exception": false,
     "start_time": "2022-04-19T11:38:09.060737",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "standardized_data = StandardScaler().fit_transform(train_df_for_pca)\n",
    "print(standardized_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.036049,
     "end_time": "2022-04-19T11:38:09.917122",
     "exception": false,
     "start_time": "2022-04-19T11:38:09.881073",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "#### Step-B: Now build the c-variance matrix which is :\n",
    "\n",
    "### $A^T * A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:09.995112Z",
     "iopub.status.busy": "2022-04-19T11:38:09.994051Z",
     "iopub.status.idle": "2022-04-19T11:38:10.408998Z",
     "shell.execute_reply": "2022-04-19T11:38:10.408380Z",
     "shell.execute_reply.started": "2022-04-19T11:28:55.937061Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.455582,
     "end_time": "2022-04-19T11:38:10.409125",
     "exception": false,
     "start_time": "2022-04-19T11:38:09.953543",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_data = standardized_data\n",
    "\n",
    "# Matrix multiplication with numpy\n",
    "covariance_matrix = np.matmul(sample_data.T, sample_data)\n",
    "\n",
    "# As the sample_data has 784 columns, so the co-variance matrix shape\n",
    "# should be 784 * 784\n",
    "print('Shape of Co-variance matrix = ', covariance_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036706,
     "end_time": "2022-04-19T11:38:10.482591",
     "exception": false,
     "start_time": "2022-04-19T11:38:10.445885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Step-C => Get the EigneVectors of this Covariance-Matrix S for projecting onto a 2-D space. These Eigenvectors will be sorted by the value of λ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:10.561253Z",
     "iopub.status.busy": "2022-04-19T11:38:10.560706Z",
     "iopub.status.idle": "2022-04-19T11:38:10.650274Z",
     "shell.execute_reply": "2022-04-19T11:38:10.651200Z",
     "shell.execute_reply.started": "2022-04-19T11:28:56.451720Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.132161,
     "end_time": "2022-04-19T11:38:10.651361",
     "exception": false,
     "start_time": "2022-04-19T11:38:10.519200",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import eigh\n",
    "\n",
    "eigenvalues, eigenvectors = eigh(covariance_matrix, eigvals=(782, 783))\n",
    "print('Shape of Eigenvectors ', eigenvectors.shape )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037467,
     "end_time": "2022-04-19T11:38:10.727457",
     "exception": false,
     "start_time": "2022-04-19T11:38:10.689990",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "So you have the principal components. They are eigenvectors of the _covariance_ matrix $X^T X$.\n",
    "\n",
    "A way to retrieve the eigenvalues from there is to apply this matrix to each principal components and project the results onto the component.\n",
    "Let v_1  be the first principal component and lambda_1 the associated eigenvalue. We have:\n",
    "[![eq][1]][1] and thus:\n",
    "[![eq2][2]][2] since [![eq3][3]][3]. (x, y)  the scalar product of vectors x and y.\n",
    "\n",
    "[1]: http://i.stack.imgur.com/6OApA.gif\n",
    "[2]: http://i.stack.imgur.com/iCZcI.gif\n",
    "[3]: http://i.stack.imgur.com/8pGd1.gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:10.806890Z",
     "iopub.status.busy": "2022-04-19T11:38:10.805263Z",
     "iopub.status.idle": "2022-04-19T11:38:10.807503Z",
     "shell.execute_reply": "2022-04-19T11:38:10.807943Z",
     "shell.execute_reply.started": "2022-04-19T11:28:56.554074Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.043013,
     "end_time": "2022-04-19T11:38:10.808053",
     "exception": false,
     "start_time": "2022-04-19T11:38:10.765040",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now converting the eigenvectors into (2, d) shape for ease of computation further ahead\n",
    "eigenvectors = eigenvectors.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036998,
     "end_time": "2022-04-19T11:38:10.881491",
     "exception": false,
     "start_time": "2022-04-19T11:38:10.844493",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    " #### eigenvectors[0] represents the eigenvector corresponding to 1st Principal Component\n",
    " #### eigenvectors[1] represents the eigenvector corresponding to 2nd Principal Component\n",
    "\n",
    "Now finally our target which was to reduce the 784-dimension data to 2-dimension data and plot it.\n",
    "\n",
    "#### Step*D => Now I need to project the original data sample on the plane formed by two principle eigenvectors by vector-vector multiplication.\n",
    "\n",
    "#### Simplest Formulate to Refer for Vector Projection\n",
    "\n",
    "![img](https://i.imgur.com/enenSdD.png)\n",
    "\n",
    "[Source](https://users.math.msu.edu/users/gnagy/teaching/11-fall/mth234/L03-234.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:10.962057Z",
     "iopub.status.busy": "2022-04-19T11:38:10.960576Z",
     "iopub.status.idle": "2022-04-19T11:38:10.990596Z",
     "shell.execute_reply": "2022-04-19T11:38:10.990133Z",
     "shell.execute_reply.started": "2022-04-19T11:28:56.561330Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.072211,
     "end_time": "2022-04-19T11:38:10.990712",
     "exception": false,
     "start_time": "2022-04-19T11:38:10.918501",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "projected_vec = np.matmul(eigenvectors, sample_data.T)\n",
    "projected_vec.shape\n",
    "# Now we will see this is a 2-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036853,
     "end_time": "2022-04-19T11:38:11.067025",
     "exception": false,
     "start_time": "2022-04-19T11:38:11.030172",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "Now I will use vstack() function to Stack arrays in sequence vertically (row wise). This function makes most sense for arrays with up to 3 dimensions. For instance, for pixel-data with a height (first axis), width (second axis), and r/g/b channels (third axis).\n",
    "\n",
    "```python\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([2, 3, 4])\n",
    "np.vstack((a,b))\n",
    "array([[1, 2, 3],\n",
    "       [2, 3, 4]])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:11.145954Z",
     "iopub.status.busy": "2022-04-19T11:38:11.144824Z",
     "iopub.status.idle": "2022-04-19T11:38:11.149927Z",
     "shell.execute_reply": "2022-04-19T11:38:11.149482Z",
     "shell.execute_reply.started": "2022-04-19T11:28:56.601404Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.046006,
     "end_time": "2022-04-19T11:38:11.150018",
     "exception": false,
     "start_time": "2022-04-19T11:38:11.104012",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# appending label to the 2-d projected data\n",
    "projected_vec = np.vstack((projected_vec, labels)).T\n",
    "projected_vec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:11.234710Z",
     "iopub.status.busy": "2022-04-19T11:38:11.233822Z",
     "iopub.status.idle": "2022-04-19T11:38:11.236658Z",
     "shell.execute_reply": "2022-04-19T11:38:11.237060Z",
     "shell.execute_reply.started": "2022-04-19T11:28:56.612545Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.050153,
     "end_time": "2022-04-19T11:38:11.237155",
     "exception": false,
     "start_time": "2022-04-19T11:38:11.187002",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The projected_vec at this point is still in array form and so we can not apply\n",
    "# head() function to it directly and also for plotting purpose, I need to make a dataframe from it.\n",
    "# creating a new dataframe from plotting the labeled points\n",
    "pca_dataframe = pd.DataFrame(data=projected_vec, columns=('1st_principal_comp', \"2nd_principal_comp\", 'labels') )\n",
    "pca_dataframe.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.037724,
     "end_time": "2022-04-19T11:38:11.313465",
     "exception": false,
     "start_time": "2022-04-19T11:38:11.275741",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "So in above each row is x_i which was originally 784 dimensions and now they are in 2-dimensions by applying PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:11.393043Z",
     "iopub.status.busy": "2022-04-19T11:38:11.392411Z",
     "iopub.status.idle": "2022-04-19T11:38:12.145039Z",
     "shell.execute_reply": "2022-04-19T11:38:12.145462Z",
     "shell.execute_reply.started": "2022-04-19T11:28:56.631245Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.794211,
     "end_time": "2022-04-19T11:38:12.145582",
     "exception": false,
     "start_time": "2022-04-19T11:38:11.351371",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.FacetGrid(pca_dataframe, hue='labels', size=6).map(plt.scatter, '1st_principal_comp', \"2nd_principal_comp\").add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.039855,
     "end_time": "2022-04-19T11:38:12.225914",
     "exception": false,
     "start_time": "2022-04-19T11:38:12.186059",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "And we can see some degree of classifications among the digits. For example, all zeros are on the top left corner. All the sixes (oranges) are towards the bottom between 0 and 4 on the x-axis.\n",
    "\n",
    "# Now deriving PCA with scikit-learn's PCA function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:12.312016Z",
     "iopub.status.busy": "2022-04-19T11:38:12.311019Z",
     "iopub.status.idle": "2022-04-19T11:38:13.657782Z",
     "shell.execute_reply": "2022-04-19T11:38:13.658304Z",
     "shell.execute_reply.started": "2022-04-19T11:28:57.564768Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1.392348,
     "end_time": "2022-04-19T11:38:13.658450",
     "exception": false,
     "start_time": "2022-04-19T11:38:12.266102",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca = decomposition.PCA()\n",
    "pca.n_components = 2\n",
    "pca_data_with_scikit = pca.fit_transform(standardized_data)\n",
    "pca_data_with_scikit.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:13.747541Z",
     "iopub.status.busy": "2022-04-19T11:38:13.746707Z",
     "iopub.status.idle": "2022-04-19T11:38:13.751880Z",
     "shell.execute_reply": "2022-04-19T11:38:13.751432Z",
     "shell.execute_reply.started": "2022-04-19T11:28:59.002829Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.051925,
     "end_time": "2022-04-19T11:38:13.751985",
     "exception": false,
     "start_time": "2022-04-19T11:38:13.700060",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pca_data_with_scikit = np.vstack((pca_data_with_scikit.T, labels)).T\n",
    "pca_data_with_scikit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:13.842576Z",
     "iopub.status.busy": "2022-04-19T11:38:13.841763Z",
     "iopub.status.idle": "2022-04-19T11:38:13.845558Z",
     "shell.execute_reply": "2022-04-19T11:38:13.845155Z",
     "shell.execute_reply.started": "2022-04-19T11:28:59.013001Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.053185,
     "end_time": "2022-04-19T11:38:13.845671",
     "exception": false,
     "start_time": "2022-04-19T11:38:13.792486",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_PCA_scikit = pd.DataFrame(data=pca_data_with_scikit, columns=('f1_PC', 'f2_PC', 'labels'))\n",
    "df_PCA_scikit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:13.933971Z",
     "iopub.status.busy": "2022-04-19T11:38:13.933074Z",
     "iopub.status.idle": "2022-04-19T11:38:14.657632Z",
     "shell.execute_reply": "2022-04-19T11:38:14.658062Z",
     "shell.execute_reply.started": "2022-04-19T11:28:59.031913Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.770526,
     "end_time": "2022-04-19T11:38:14.658182",
     "exception": false,
     "start_time": "2022-04-19T11:38:13.887656",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.FacetGrid(df_PCA_scikit, hue='labels', size=6).map(plt.scatter, 'f1_PC', 'f2_PC').add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.043975,
     "end_time": "2022-04-19T11:38:14.746369",
     "exception": false,
     "start_time": "2022-04-19T11:38:14.702394",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "And I am getting almost similar plotting that we got with the manual steps to calculate Principle Components. Main difference with sickit-learn the axes are reversed.\n",
    "\n",
    "\n",
    "# Plot to show how Variance Retained increases as we increase the Number of Principle components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:14.839292Z",
     "iopub.status.busy": "2022-04-19T11:38:14.838389Z",
     "iopub.status.idle": "2022-04-19T11:38:20.894407Z",
     "shell.execute_reply": "2022-04-19T11:38:20.893547Z",
     "shell.execute_reply.started": "2022-04-19T11:28:59.937884Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 6.104741,
     "end_time": "2022-04-19T11:38:20.894517",
     "exception": false,
     "start_time": "2022-04-19T11:38:14.789776",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Shape of standardized_data ', standardized_data.shape)\n",
    "\n",
    "pca.n_components = 784\n",
    "pca_data_scikit_2 = pca.fit_transform(standardized_data)\n",
    "percent_variance_retained = pca.explained_variance_ / np.sum(pca.explained_variance_)\n",
    "cumulative_variance_retained = np.cumsum(percent_variance_retained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T11:38:21.082064Z",
     "iopub.status.busy": "2022-04-19T11:38:21.081168Z",
     "iopub.status.idle": "2022-04-19T11:38:21.397345Z",
     "shell.execute_reply": "2022-04-19T11:38:21.396630Z",
     "shell.execute_reply.started": "2022-04-19T11:29:07.129388Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 0.371042,
     "end_time": "2022-04-19T11:38:21.397454",
     "exception": false,
     "start_time": "2022-04-19T11:38:21.026412",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10, 6))\n",
    "plt.clf()\n",
    "plt.plot(cumulative_variance_retained, linewidth=2)\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative variance Retained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.04473,
     "end_time": "2022-04-19T11:38:21.489161",
     "exception": false,
     "start_time": "2022-04-19T11:38:21.444431",
     "status": "completed"
    },
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "So above was all about Dimensionality Reduction and PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>4.456643</td>\n",
       "      <td>2.887730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel0</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel1</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel2</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel3</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel94</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>3.768381</td>\n",
       "      <td>26.956990</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel95</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>5.713881</td>\n",
       "      <td>33.283039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel96</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>7.751238</td>\n",
       "      <td>38.610092</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel97</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>10.048857</td>\n",
       "      <td>44.044693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pixel98</th>\n",
       "      <td>42000.0</td>\n",
       "      <td>12.067738</td>\n",
       "      <td>48.119860</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           count       mean        std  min  25%  50%  75%    max\n",
       "label    42000.0   4.456643   2.887730  0.0  2.0  4.0  7.0    9.0\n",
       "pixel0   42000.0   0.000000   0.000000  0.0  0.0  0.0  0.0    0.0\n",
       "pixel1   42000.0   0.000000   0.000000  0.0  0.0  0.0  0.0    0.0\n",
       "pixel2   42000.0   0.000000   0.000000  0.0  0.0  0.0  0.0    0.0\n",
       "pixel3   42000.0   0.000000   0.000000  0.0  0.0  0.0  0.0    0.0\n",
       "...          ...        ...        ...  ...  ...  ...  ...    ...\n",
       "pixel94  42000.0   3.768381  26.956990  0.0  0.0  0.0  0.0  255.0\n",
       "pixel95  42000.0   5.713881  33.283039  0.0  0.0  0.0  0.0  255.0\n",
       "pixel96  42000.0   7.751238  38.610092  0.0  0.0  0.0  0.0  255.0\n",
       "pixel97  42000.0  10.048857  44.044693  0.0  0.0  0.0  0.0  255.0\n",
       "pixel98  42000.0  12.067738  48.119860  0.0  0.0  0.0  0.0  255.0\n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_org.describe().T[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "papermill": {
   "duration": 528.169307,
   "end_time": "2022-04-19T11:46:46.217044",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-04-19T11:37:58.047737",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
